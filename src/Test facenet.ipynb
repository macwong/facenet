{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/davidsandberg/facenet/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import facenet\n",
    "import lfw\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import brentq\n",
    "from scipy import interpolate\n",
    "\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model = '../data/models/20170512-110547.pb', lfw_dir = '../data/lfw/lfw_mtcnnpy_160', lfw_batch_size = 100, image_size = 160, lfw_pairs = '../data/pairs.txt', lfw_file_ext = 'png', lfw_nrof_folds = 10):\n",
    "  \n",
    "    with tf.Graph().as_default():\n",
    "      \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Read the file containing the pairs used for testing\n",
    "            pairs = lfw.read_pairs(os.path.expanduser(lfw_pairs))\n",
    "\n",
    "            # Get the paths for the corresponding images\n",
    "            paths, actual_issame = lfw.get_paths(os.path.expanduser(lfw_dir), pairs, lfw_file_ext)\n",
    "\n",
    "            # Load the model\n",
    "            facenet.load_model(model)\n",
    "            \n",
    "            # Get input and output tensors\n",
    "            images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "            embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "            \n",
    "            #image_size = images_placeholder.get_shape()[1]  # For some reason this doesn't work for frozen graphs\n",
    "            image_size = image_size\n",
    "            embedding_size = embeddings.get_shape()[1]\n",
    "        \n",
    "            # Run forward pass to calculate embeddings\n",
    "            print('Runnning forward pass on LFW images')\n",
    "            batch_size = lfw_batch_size\n",
    "            nrof_images = len(paths)\n",
    "            nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "            emb_array = np.zeros((nrof_images, embedding_size))\n",
    "            for i in range(nrof_batches):\n",
    "                start_index = i*batch_size\n",
    "                end_index = min((i+1)*batch_size, nrof_images)\n",
    "                paths_batch = paths[start_index:end_index]\n",
    "                images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "        \n",
    "            tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(emb_array, \n",
    "                actual_issame, nrof_folds=lfw_nrof_folds)\n",
    "\n",
    "            print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "            print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            print('Area Under Curve (AUC): %1.3f' % auc)\n",
    "            eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n",
    "            print('Equal Error Rate (EER): %1.3f' % eer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model filename: ../data/models/20170512-110547.pb\n",
      "Runnning forward pass on LFW images\n",
      "Accuracy: 0.992+-0.003\n",
      "Validation rate: 0.97467+-0.01477 @ FAR=0.00133\n",
      "Area Under Curve (AUC): 1.000\n",
      "Equal Error Rate (EER): 0.007\n"
     ]
    }
   ],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d4875203a69e>:31: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(cls.image_paths)>0, 'There must be at least one image for each class in the dataset')\n"
     ]
    }
   ],
   "source": [
    "def classifier(mode, # = 'CLASSIFY', \n",
    "               data_dir, # = '../data/subset/train', \n",
    "               classifier_filename, # = '../data/subset/subset_classifier.pkl', \n",
    "               model = '../data/models/20170512-110547.pb', \n",
    "               use_split_dataset = False, \n",
    "               test_data_dir = '../data/subset/test', \n",
    "               batch_size=90, \n",
    "               image_size=160, \n",
    "               seed=666, \n",
    "               min_nrof_images_per_class=20, \n",
    "               nrof_train_images_per_class=10):\n",
    "  \n",
    "    with tf.Graph().as_default():\n",
    "      \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            np.random.seed(seed=seed)\n",
    "            \n",
    "            if use_split_dataset:\n",
    "                dataset_tmp = facenet.get_dataset(data_dir)\n",
    "                train_set, test_set = split_dataset(dataset_tmp, min_nrof_images_per_class, nrof_train_images_per_class)\n",
    "                if (mode=='TRAIN'):\n",
    "                    dataset = train_set\n",
    "                elif (mode=='CLASSIFY'):\n",
    "                    dataset = test_set\n",
    "            else:\n",
    "                dataset = facenet.get_dataset(data_dir)\n",
    "\n",
    "            # Check that there are at least one training image per class\n",
    "            for cls in dataset:\n",
    "                assert(len(cls.image_paths)>0, 'There must be at least one image for each class in the dataset')            \n",
    "\n",
    "                 \n",
    "            paths, labels = facenet.get_image_paths_and_labels(dataset)\n",
    "            \n",
    "            print('Number of classes: %d' % len(dataset))\n",
    "            print('Number of images: %d' % len(paths))\n",
    "            \n",
    "            # Load the model\n",
    "            print('Loading feature extraction model')\n",
    "            facenet.load_model(model)\n",
    "            \n",
    "            # Get input and output tensors\n",
    "            images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "            embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "            embedding_size = embeddings.get_shape()[1]\n",
    "            \n",
    "            # Run forward pass to calculate embeddings\n",
    "            print('Calculating features for images')\n",
    "            nrof_images = len(paths)\n",
    "            nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "            emb_array = np.zeros((nrof_images, embedding_size))\n",
    "            for i in range(nrof_batches_per_epoch):\n",
    "                start_index = i*batch_size\n",
    "                end_index = min((i+1)*batch_size, nrof_images)\n",
    "                paths_batch = paths[start_index:end_index]\n",
    "                images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "            \n",
    "            classifier_filename_exp = os.path.expanduser(classifier_filename)\n",
    "\n",
    "            if (mode=='TRAIN'):\n",
    "                # Train classifier\n",
    "                print('Training classifier')\n",
    "                model = SVC(kernel='linear', probability=True)\n",
    "                model.fit(emb_array, labels)\n",
    "            \n",
    "                # Create a list of class names\n",
    "                class_names = [ cls.name.replace('_', ' ') for cls in dataset]\n",
    "\n",
    "                # Saving classifier model\n",
    "                with open(classifier_filename_exp, 'wb') as outfile:\n",
    "                    pickle.dump((model, class_names), outfile)\n",
    "                print('Saved classifier model to file \"%s\"' % classifier_filename_exp)\n",
    "                \n",
    "            elif (mode=='CLASSIFY'):\n",
    "                # Classify images\n",
    "                print('Testing classifier')\n",
    "                with open(classifier_filename_exp, 'rb') as infile:\n",
    "                    (model, class_names) = pickle.load(infile)\n",
    "\n",
    "                print('Loaded classifier model from file \"%s\"' % classifier_filename_exp)\n",
    "\n",
    "                predictions = model.predict_proba(emb_array)\n",
    "                best_class_indices = np.argmax(predictions, axis=1)\n",
    "                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n",
    "                \n",
    "                for i in range(len(best_class_indices)):\n",
    "                    print('%4d  %s: %.3f' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n",
    "#                     print(predictions[i])\n",
    "#                     print(paths[i])\n",
    "                    \n",
    "                accuracy = np.mean(np.equal(best_class_indices, labels))\n",
    "                print('Accuracy: %.3f' % accuracy)\n",
    "                \n",
    "            \n",
    "def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for cls in dataset:\n",
    "        paths = cls.image_paths\n",
    "        # Remove classes with less than min_nrof_images_per_class\n",
    "        if len(paths)>=min_nrof_images_per_class:\n",
    "            np.random.shuffle(paths)\n",
    "            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n",
    "            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9\n",
      "Number of images: 18\n",
      "Loading feature extraction model\n",
      "Model filename: ../data/models/20170512-110547.pb\n",
      "Calculating features for images\n",
      "Training classifier\n",
      "Saved classifier model to file \"../data/subset_small/subset_classifier.pkl\"\n"
     ]
    }
   ],
   "source": [
    "# classifier(mode = \"TRAIN\", \n",
    "#            data_dir = '../data/lfw/lfw_mtcnnpy_160',\n",
    "#            classifier_filename = '../data/lfw/classifier.pkl')\n",
    "\n",
    "# classifier(mode = \"TRAIN\", \n",
    "#            data_dir = '../data/subset/train',\n",
    "#            classifier_filename = '../data/subset/classifier.pkl')\n",
    "\n",
    "classifier(mode = \"TRAIN\", \n",
    "           data_dir = '../data/subset_small/train', \n",
    "           classifier_filename = '../data/subset_small/subset_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 9\n",
      "Number of images: 9\n",
      "Loading feature extraction model\n",
      "Model filename: ../data/models/20170512-110547.pb\n",
      "Calculating features for images\n",
      "Testing classifier\n",
      "Loaded classifier model from file \"../data/subset_small/subset_classifier.pkl\"\n",
      "   0  Al Pacino: 0.215\n",
      "   1  Ben Affleck: 0.214\n",
      "   2  Britney Spears: 0.221\n",
      "   3  Halle Berry: 0.279\n",
      "   4  Harbhajan Singh: 0.329\n",
      "   5  Oprah Winfrey: 0.207\n",
      "   6  Will Smith: 0.247\n",
      "   7  Winona Ryder: 0.212\n",
      "   8  Yao Ming: 0.276\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# classifier(mode = 'CLASSIFY', \n",
    "#            data_dir = '../data/subset/train', \n",
    "#            classifier_filename = '../data/lfw/classifier.pkl')\n",
    "\n",
    "# classifier(mode = 'CLASSIFY', \n",
    "#            data_dir = '../data/subset/test',\n",
    "#            classifier_filename = '../data/subset/subset_classifier.pkl')\n",
    "\n",
    "classifier(mode = 'CLASSIFY', \n",
    "           data_dir = '../data/subset_small/test', \n",
    "           classifier_filename = '../data/subset_small/subset_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
